{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoders for recommender systems\n",
    "Recommendations based on Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some paths to download data\n",
    "ratingsPath = \"../data/ml-latest-small/ratings.csv\"\n",
    "moviesPath = \"../data/ml-latest-small/movies.csv\"\n",
    "\n",
    "# define reader instance to download data\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "# download dataset to path\n",
    "ratingsDataset = Dataset.load_from_file(ratingsPath, reader=reader)\n",
    "\n",
    "# now parse movies dataset\n",
    "movieID_to_name = {}\n",
    "name_to_movieID = {}\n",
    "with open(moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
    "    movieReader = csv.reader(csvfile)\n",
    "    next(movieReader)\n",
    "    for row in movieReader:\n",
    "        movieID = int(row[0])\n",
    "        movieName = row[1]\n",
    "        movieID_to_name[movieID] = movieName\n",
    "        name_to_movieID[movieName] = movieID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get user ratings from ratingsDataset based on user\n",
    "def getUserRatings(user):\n",
    "    userRatings = []\n",
    "    hitUser = False\n",
    "    with open(ratingsPath, newline='') as csvfile:\n",
    "        ratingReader = csv.reader(csvfile)\n",
    "        next(ratingReader)\n",
    "        for row in ratingReader:\n",
    "            userID = int(row[0])\n",
    "            if (user == userID):\n",
    "                movieID = int(row[1])\n",
    "                rating = float(row[2])\n",
    "                userRatings.append((movieID, rating))\n",
    "                hitUser = True\n",
    "            if (hitUser and (user != userID)):\n",
    "                break\n",
    "\n",
    "    return userRatings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it is important to get popularity ranks to get some metrics\n",
    "ratings = defaultdict(int)\n",
    "rankings = defaultdict(int)\n",
    "with open(ratingsPath, newline='') as csvfile:\n",
    "    ratingReader = csv.reader(csvfile)\n",
    "    next(ratingReader)\n",
    "    for row in ratingReader:\n",
    "        movieID = int(row[1])\n",
    "        ratings[movieID] += 1\n",
    "    rank = 1\n",
    "    for movieID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
    "        rankings[movieID] = rank\n",
    "        rank +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the AutoEncoder arquitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRec(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            visibleDimensions,\n",
    "            epochs=200,\n",
    "            hiddenDimensions=50,\n",
    "            learningRate=0.1,\n",
    "            batchSize=100\n",
    "            ):\n",
    "        \n",
    "       self.visibleDimensions = visibleDimensions\n",
    "       self.epochs = epochs\n",
    "       self.hiddenDimensions = hiddenDimensions\n",
    "       self.learningRate = learningRate\n",
    "       self.batchSize = batchSize\n",
    "       self.optimizer = tf.keras.optimizers.RMSprop(self.learningRate)\n",
    "\n",
    "    def Train(self, X):\n",
    "        self.initialize_weights_biases()\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, X.shape[0], self.batchSize):\n",
    "                epochX = X[i:i+self.batchSize]\n",
    "                self.run_optimization(epochX)\n",
    "            print(\"Trained epoch\", epoch)\n",
    "    \n",
    "    def GetRecommendations(self, inputUser):\n",
    "        # feed a single user and get output from the output layer\n",
    "        rec = self.neural_net(inputUser)\n",
    "        # return type is a eager tensor\n",
    "        return rec[0]\n",
    "    \n",
    "    def initialize_weights_biases(self,):\n",
    "        # random initialize weights for hidden and output\n",
    "        self.weights = {\n",
    "            'h1': tf.Variable(tf.random.normal([self.visibleDimensions, self.hiddenDimensions])),\n",
    "            'out': tf.Variable(tf.random.normal([self.hiddenDimensions, self.visibleDimensions]))\n",
    "        }\n",
    "        # same for biases\n",
    "        self.biases = {\n",
    "            'b1': tf.Variable(tf.random.normal([self.hiddenDimensions])),\n",
    "            'out': tf.Variable(tf.random.normal([self.visibleDimensions]))\n",
    "        }\n",
    "    \n",
    "    def neural_net(self, inputUser):\n",
    "        # define arquitecture\n",
    "        # input layer\n",
    "        self.inputLayer = inputUser\n",
    "\n",
    "        # hidden layer\n",
    "        hidden = tf.nn.sigmoid(tf.add(tf.matmul(self.inputLayer, self.weights['h1']), self.biases['b1']))\n",
    "        # output layer for predictions\n",
    "        self.outputLayer = tf.nn.sigmoid(tf.add(tf.matmul(hidden, self.weights['out']), self.biases['out']))\n",
    "\n",
    "        return self.outputLayer\n",
    "\n",
    "    def run_optimization(self, inputUser):\n",
    "        with tf.GradientTape() as g:\n",
    "            pred = self.neural_net(inputUser)\n",
    "            loss = tf.keras.losses.MSE(inputUser, pred)\n",
    "\n",
    "        trainable_variables = list(self.weights.values()) + list(self.biases.values())\n",
    "        gradients = g.gradient(loss, trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AutoRec wrapper with surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRecAlgorithm(AlgoBase):\n",
    "    def __init__(self, epochs=100, hiddenDim=100, learningRate=0.01, batchSize=100, sim_options={}):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.epochs = epochs\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        numUsers = trainset.n_users\n",
    "        numItems = trainset.n_items\n",
    "\n",
    "        trainingMatrix = np.zeros([numUsers, numItems,], dtype=np.float32)\n",
    "        for (uid, iid, rating) in trainset.all_ratings():\n",
    "            # normalized ratings for not using softmax\n",
    "            trainingMatrix[int(uid), int(iid)] = rating / 5.0\n",
    "\n",
    "        # create RBM with (n_items * rating_values) visible nodes\n",
    "        autoRec = AutoRec(\n",
    "            trainingMatrix.shape[1],\n",
    "            hiddenDimensions=self.hiddenDim,\n",
    "            learningRate=self.learningRate,\n",
    "            batchSize=self.batchSize,\n",
    "            epochs=self.epochs\n",
    "        )\n",
    "        autoRec.Train(trainingMatrix)\n",
    "        \n",
    "        self.predictedRatings = np.zeros([numUsers, numItems], dtype=np.float32)\n",
    "        for uiid in range(trainset.n_users):\n",
    "            if (uiid % 50 == 0):\n",
    "                print(\"Processing user \", uiid)\n",
    "            recs = autoRec.GetRecommendations([trainingMatrix[uiid]])\n",
    "            # take a normalized rating and re-escale it\n",
    "            for itemID, rec in enumerate(recs):\n",
    "                self.predictedRatings[uiid, itemID] = rec * 0.5\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unknown')\n",
    "        \n",
    "        rating = self.predictedRatings[u, i]\n",
    "        if (rating < 0.001):\n",
    "            raise PredictionImpossible('No valid prediction exists')\n",
    "        \n",
    "        return rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some recomendations based on AutoRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from Framework.EvaluationData import EvaluationData\n",
    "from Framework.RecommenderMetrics import RecommenderMetrics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Trained epoch 0\n",
      "Trained epoch 1\n",
      "Trained epoch 2\n",
      "Trained epoch 3\n",
      "Trained epoch 4\n",
      "Trained epoch 5\n",
      "Trained epoch 6\n",
      "Trained epoch 7\n",
      "Trained epoch 8\n",
      "Trained epoch 9\n",
      "Trained epoch 10\n",
      "Trained epoch 11\n",
      "Trained epoch 12\n",
      "Trained epoch 13\n",
      "Trained epoch 14\n",
      "Trained epoch 15\n",
      "Trained epoch 16\n",
      "Trained epoch 17\n",
      "Trained epoch 18\n",
      "Trained epoch 19\n",
      "Trained epoch 20\n",
      "Trained epoch 21\n",
      "Trained epoch 22\n",
      "Trained epoch 23\n",
      "Trained epoch 24\n",
      "Trained epoch 25\n",
      "Trained epoch 26\n",
      "Trained epoch 27\n",
      "Trained epoch 28\n",
      "Trained epoch 29\n",
      "Trained epoch 30\n",
      "Trained epoch 31\n",
      "Trained epoch 32\n",
      "Trained epoch 33\n",
      "Trained epoch 34\n",
      "Trained epoch 35\n",
      "Trained epoch 36\n",
      "Trained epoch 37\n",
      "Trained epoch 38\n",
      "Trained epoch 39\n",
      "Trained epoch 40\n",
      "Trained epoch 41\n",
      "Trained epoch 42\n",
      "Trained epoch 43\n",
      "Trained epoch 44\n",
      "Trained epoch 45\n",
      "Trained epoch 46\n",
      "Trained epoch 47\n",
      "Trained epoch 48\n",
      "Trained epoch 49\n",
      "Trained epoch 50\n",
      "Trained epoch 51\n",
      "Trained epoch 52\n",
      "Trained epoch 53\n",
      "Trained epoch 54\n",
      "Trained epoch 55\n",
      "Trained epoch 56\n",
      "Trained epoch 57\n",
      "Trained epoch 58\n",
      "Trained epoch 59\n",
      "Trained epoch 60\n",
      "Trained epoch 61\n",
      "Trained epoch 62\n",
      "Trained epoch 63\n",
      "Trained epoch 64\n",
      "Trained epoch 65\n",
      "Trained epoch 66\n",
      "Trained epoch 67\n",
      "Trained epoch 68\n",
      "Trained epoch 69\n",
      "Trained epoch 70\n",
      "Trained epoch 71\n",
      "Trained epoch 72\n",
      "Trained epoch 73\n",
      "Trained epoch 74\n",
      "Trained epoch 75\n",
      "Trained epoch 76\n",
      "Trained epoch 77\n",
      "Trained epoch 78\n",
      "Trained epoch 79\n",
      "Trained epoch 80\n",
      "Trained epoch 81\n",
      "Trained epoch 82\n",
      "Trained epoch 83\n",
      "Trained epoch 84\n",
      "Trained epoch 85\n",
      "Trained epoch 86\n",
      "Trained epoch 87\n",
      "Trained epoch 88\n",
      "Trained epoch 89\n",
      "Trained epoch 90\n",
      "Trained epoch 91\n",
      "Trained epoch 92\n",
      "Trained epoch 93\n",
      "Trained epoch 94\n",
      "Trained epoch 95\n",
      "Trained epoch 96\n",
      "Trained epoch 97\n",
      "Trained epoch 98\n",
      "Trained epoch 99\n",
      "Processing user  0\n",
      "Processing user  50\n",
      "Processing user  100\n",
      "Processing user  150\n",
      "Processing user  200\n",
      "Processing user  250\n",
      "Processing user  300\n",
      "Processing user  350\n",
      "Processing user  400\n",
      "Processing user  450\n",
      "Processing user  500\n",
      "Processing user  550\n",
      "Processing user  600\n",
      "RMSE SVD 1.5170175780134585\n",
      "MAE SVD 1.1321810200658786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define evaluation data\n",
    "# get evaluation data\n",
    "evaluationData = EvaluationData(ratingsDataset, rankings)\n",
    "# define AutoRec instance\n",
    "autorec = AutoRecAlgorithm()\n",
    "autorec.fit(ratingsDataset.build_full_trainset())\n",
    "predictions = autorec.test(evaluationData.GetTestSet())\n",
    "print(\"RMSE SVD\", RecommenderMetrics.RMSE(predictions))\n",
    "print(\"MAE SVD\", RecommenderMetrics.MAE(predictions))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to ger movie name based on movie ID\n",
    "def getMovieName(movieID):\n",
    "    if movieID in movieID_to_name:\n",
    "        return movieID_to_name[movieID]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## AutoRec recommendations ##########\n",
      "Grumpier Old Men (1995) 3.501556983616962\n",
      "Heat (1995) 3.501556983616962\n",
      "Seven (a.k.a. Se7en) (1995) 3.501556983616962\n",
      "Usual Suspects, The (1995) 3.501556983616962\n",
      "From Dusk Till Dawn (1996) 3.501556983616962\n",
      "Bottle Rocket (1996) 3.501556983616962\n",
      "Rob Roy (1995) 3.501556983616962\n",
      "Desperado (1995) 3.501556983616962\n",
      "Clerks (1994) 3.501556983616962\n",
      "Dumb & Dumber (Dumb and Dumber) (1994) 3.501556983616962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see recomendations\n",
    "# let's see some recommendations\n",
    "testSubject = 85\n",
    "k = 10\n",
    "\n",
    "trainSet = evaluationData.GetFullTrainSet()\n",
    "testSet = evaluationData.GetAntiTestSetForUser(testSubject)\n",
    "\n",
    "predictions = autorec.test(testSet)\n",
    "recommendations = []\n",
    "for userID, movieID, actualRating, EstimatedRating, _ in predictions:\n",
    "    intMovieID = int(movieID)\n",
    "    recommendations.append((intMovieID, EstimatedRating))\n",
    "\n",
    "recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"#\"*10, \"AutoRec recommendations\", \"#\"*10)\n",
    "for ratings in recommendations[:k]:\n",
    "    print(getMovieName(ratings[0]), ratings[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a param grid\n",
    "param_grid = {'hiddenDim': [20, 10], 'learningRate': [0.1, 0.01]}\n",
    "gs = GridSearchCV(AutoRecAlgorithm, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "gs.fit(ratingsDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score attained:  1.1342662605735316\n",
      "{'hiddenDim': 10, 'learningRate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# now explore best metrics and params\n",
    "print(\"Best RMSE score attained: \", gs.best_score['rmse'])\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender_systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
